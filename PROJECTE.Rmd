---
title: "DELIVERABLE 1"
author: "Carles Aguilera , Joel Delgado, Oriol Vilella"
date: "2025-10-08"
output:
  word_document: default
  html_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
```

```{r}
df <- read.csv("RealEstate_Georgia.csv")
View(df)
df_old <- df
```


# DATA PREPARATION

The first step of a data science project involves cleaning the raw data set. In this section we address how to deal with missing values, correcting inconsistencies and transforming the data into a format suitable for algorithms. Therefore, in this section, we will study each variable in order to clean as much as possible the dataset for the future results to be as accurate as possible.

## Variable 1: *stateId*

Although *stateId* seems to be a nominal variable since it represents a state category, it is not really a variable because its value never changes: All rows have *stateId* = 16, which stands for Georgia state. Therefore, this column does not provide any relevant information to any statistical model, so it can be removed from our dataframe.
```{r}
summary(df$stateId)
which(df$stateId != "16")
df <- subset(df, select = -stateId)
```

## Variable 2: countyId

The variable *countyId* is a nominal variable with numeric values that are used to identify the county where a house is. Also, there is another nominal variable named *county* whose values are the names of the counties. When comparing these two columns an inconsistency can be noticed: We would expect rows that share the same *county* value to also share the same *countyId*, as the latter is the numerical representation of the former. However that is not the case, as the vast majority of *countyId* values are unique (only 30 of them are repeated once). As a result, we discard this column as we consider it does not provide any useful insight that is not provided by the *county* column.

```{r}
table(df$countyId)
prop.table(table(df$countyId))
length(which(table(df$countyId) > 1)) # Number of values that are repeated at least one time
length(which(table(df$countyId) > 2)) 
# IN THAT CASE WE OBSERVE WITH THAT TABLE AND THIS CONDITIONS THAT EACH VALUE APPEARS ONLY ONE TIME, AND SOMETIMES TWO TIMES, BUT NO MORE THAN 2
df <- subset(df, select = -countyId)
```

## Variable 3: cityId

The variable *cityId* is a nominal variable with numeric values. Similarly to *countyId* and *county*, there also exists a variable named *city* with the names of cities. We would assume that all rows with the same *city* value would be mapped to the same unique *cityId*, but that is not the case. As an example, there exist two rows who share a *cityId* of 17797 but belong to Atlanta and Doraville. Also, there exist two rows with *city* Dunwoody but with *cityId* 49352 and 42728.

```{r}
# Here we check that, for example, Dunwoody is associated with multiple cityIds (instead of a unique cityId)
summary(df$city)
table(df$city)
prop.table(table(df$city))

rows <- which(df$city == 'Dunwoody')
first_cityId <- df[rows[1],]$cityId
count1 <- 0
for (r in rows[2:length(rows)]) {
  if (df[r,]$cityId != first_cityId){
    count1 = count1 + 1
  }
}

# Here we check that, for example, 17797 is associated with multiple city (instead of a unique city)
rows <- which(df$cityId == 17797)
first_city <- df[rows[1],]$city
count2 <- 0
for (r in rows[2:length(rows)]) {
  if (df[r,]$city != first_city){
    count2 = count2 +1
  }
}

count1 
count2
```

Due to this inconsistency, we consider we have to choose between one of the two columns. We have decided to keep *city* and discard *cityId* as we think that *city* provides more useful information about houses' locations.

```{r}
df <- subset(df, select = -cityId)
```

## Variable 4: Country

The variable *country* is a nominal variable representing the name of the country of each house. Since the country of all the houses in the dataset is USA, we can remove that variable since it does not add any additional information.

```{r}
which(df$country != "USA")
df <- subset(df, select = -country)
```

## Variable 5: datePostedString

The variable *datePostedString* is a nominal variable that represents the date when the house was published. According to the instructions of this practice, we only need to retain the observations that were posted on **2021 only**, so we remove the rest of the rows and finally we remove the column

```{r}
df$datePostedString <- as.Date(df$datePostedString, format = "%Y-%m-%d")
df$datePostedString <- as.numeric(format(df$datePostedString, "%Y"))
size_out_of_year <- which(df$datePostedString != 2021); 
length(size_out_of_year)
df <- df[df$datePostedString == 2021,]

df <- subset(df, select = -datePostedString)
```

# Variable 6: is_bankOwned.

This variable indicates whether the property belongs to the bank or not. Since it is a binary variable, we consider it more appropriate to treat it as a categorical (qualitative) variable, distinguishing two categories: Yes and No, depending on whether the property is bank-owned or not.

For this reason, we analyzed this variable to identify which values correspond to bank-owned properties and which do not, confirming that there are only two possible values:
- 0 -> Not owned by the bank
- 1 -> Owned by the bank

```{r}
summary(df$is_bankOwned)
table(df$is_bankOwned) # This shows that there are only two values

df$is_bankOwned <- factor(df$is_bankOwned, labels = c("NO", "YES")) # LET'S TRANSFORM THAT VARIABLE TO A CATEGORICAL / FACTOR VARIABLE.
barplot(table(df$is_bankOwned), xlab="Is Bank Owned?")
table(df$is_bankOwned)
```

After transforming it into a factor, we can see easily in the barplot that only 1 observation out of the 6121 original rows has a different value (1) while the others have the same (0). Therefore, this observation does not affect significantly to the variability of the variable so it can be safely removed. 

Here we have to decide whether we declare the value for this observation as an error and impute it as a "NO" (preserving the rest of values of the observation) or we remove the observation entirely. For now, we will keep it. In both cases, the column *is_bankOwned* ceases to be a variable since all rows now have the same value, which allows us to safely delete the variable from our data frame. 

```{r}
df <- subset(df, select = -is_bankOwned)
```

# Variable 7: is_forAuction

This variable indicates whether a given house is being auctioned or not. In this case, the treatment applied is very similar to the one used previously: we checked for variability between the values 0 and 1, in order to confirm that there are houses that are auctioned and others that are not.

Once this variability was confirmed, we proceeded to transform the variable into a categorical one, assigning two levels: Yes and No, depending on whether the house belongs to an auction or not. As before, there is only one observation with a different value than the rest, so it is reasonable to remove this observation from the dataset or impute its value in order to be able to remove the column (*is_forAuction* variable). Since the row (3919) is different from the previous one (156), we keep it for now and only impute its value.

```{r}
summary(df$is_forAuction)
table(df$is_forAuction)

# WE HAVE THE SAME CASE AS BEFORE, SO LET'S TRANSFORM THAT VARIABLE TO A CATEGORICAL / FACTOR VARIABLE.

df$is_forAuction <- factor(df$is_forAuction, labels = c("NO", "YES"))
barplot(table(df$is_forAuction), xlab="Is for auction?")
table(df$is_forAuction)

# LOOK AFTER, WE CAN NOT REMOVE NOW THAT VARIABLE.
df <- subset(df, select = -is_forAuction)
```

# Variable 8: Event

*Event* is a qualitative variable that indicates the condition of the house. It is relevant information, as it allows us to know the current state of the property at any time. So, we consider it important to keep this variable for further analyses.

The first task to be performed is to convert this variable from type *char* to type *factor*, in order to analyze in future studies whether the condition of the house has any impact or relationship with its price. We also look for missing values, but there are none.

```{r}
str(df$event)
table(df$event)
# MAYBE THAT VARIABLE IS INTERESTING FOR FURTHER ANALYSIS IN DATA, LET'S TAKE AND WE'LL NOT ERASE IT.
sum(is.na(df$event))
# WE DON'T HAVE MISSING VALUES, SO THAT'S PERFECT ALSO. 
#df$event <- as.factor(df$event)
barplot(table(df$event))
```

It can be noticed that categories "Listing for removed", "Pending sale" and "Sold" account for 1.8% of the observations approximately (111/6121), so we decided to join them into a new category named "Other".

```{r}
df$event[which(df$event == "Listing removed")] <- 'Other'
df$event[which(df$event == 'Pending sale')] <- 'Other'
df$event[which(df$event == "Sold")] <- "Other"
df$event <- as.factor(df$event)
table(df$event)
prop.table(table(df$event))
barplot(table(df$event))
pie(table(df$event))
```


# variable 9: Time

The variable *time* is a numeric variable that represents the time collected. We have no specific interest in knowing the exact moment when the listing was published or when the house was for sale and we consider that this information is not essential for studying the property price. In consequence, it is not necessary to keep it. 

```{r}
df <- subset(df, select = -time)
```

# Variable 10: Price

*Price* is the target variable, so it is critical to examine it carefully. First, we will assess its data quality: ensure there are no missing values and that the records are consistent. Next, we will evaluate whether the price distribution is approximately normal and examine the presence of outliers.

Because price is the dependent variable in our study, we will conduct a more in-depth analysis: describe its distribution, identify and justify outliers, and discuss which features (e.g., property condition, area, location, auction status, etc.) may explain these extreme values.

```{r}
str(df$price)
sum(is.na(df$price))
# It's a numerical variable and does not have any missing values


# Check for normality. Plots show price does not follow a normal distribution
par(mfrow=c(1,2))
hist(df$price, freq = FALSE)
curve(dnorm(x,mean=mean(df$price), sd= sd(df$price)), lwd = 2, col ='red', add = TRUE)
qqnorm(df$price) # Since our number of observations is too high, we will check for normality visually with a qqplot
qqline(df$price)
par(mfrow=c(1,1))

# HERE WE CAN OBSERVE THAT THE TARGET VARIABLE PRICE IS NOT FOLLOWING A NORMAL DISTRIBUTION, SO LET'S CHECK IF WE APPLY SOME TRANSOFMRATIONS IF IT FOLLOWS A NORMAL DISTRIBUTUION OR NOT. 
par(mfrow = c(1,2))
hist(log(df$price), freq = FALSE)
curve(dnorm(x,mean=mean(log(df$price)), sd= sd(log(df$price))), lwd = 2, col ='red', add = TRUE)
qqnorm(log(df$price)) # Since our number of observations is too high, we will check for normality visually with a qqplot
qqline(log(df$price))

#We observe that this follows a little bit more the normality, but still also some values that are far from the perfect line, so let's check with the shapiro.test the normality.

sample <- df[1:5000,]
shapiro.test(log(sample$price))
# HERE WITH A SAMPLE, WE CAN SE THAT THE P-VALUE IS LESS THAN THE 0.05, SO THAT MEANS THAT THE PRICE IS NOT FOLLOWING A NORMAL DISTRIBUTION.

# Check for univariate outliers
boxplot(df$price)

# YES, WE HAVE SOME OUTLAYERS, SO LET'S CHECK, WHICH ONE'S ARE MILD OR SEVERE OUTLAYERS.
sum_sq <- summary(df$price); sum_sq
iqr <- sum_sq[5] - sum_sq[2]
llmild <- which(df$price < sum_sq[2] - 1.5*iqr | df$price > sum_sq[5] + 1.5*iqr) 
llsev <- which(df$price < sum_sq[2] - 3*iqr | df$price > sum_sq[5] + 3*iqr) 
length(llmild); length(llsev)

# WE HAVE. A TOTAL OF 40 SEVERE OUTLAYERS AND 239 MILD OUTLAYERS. LET'S SEE IN THE BOXPLOT.
boxplot(df$price)
abline(h = sum_sq[2] - 1.5*iqr, col='green')
abline(h = sum_sq[5] + 1.5*iqr, col='green')
abline(h = sum_sq[2] - 3*iqr, col='red')
abline(h = sum_sq[5] + 3*iqr, col='red')

# WHO ARE THE ONE'S THAT ARE OUTLAIERS IN THE PRICE, LET'S SEE. 

df[llsev,] # THE ONE'S THAT THE HOME PRICE IS ABOVE THE 1.249.000 €.
sort(df$price[llsev])
```

SO IN THAT CASE, WE CAN OBSERVE THAT THE ONE'S THAT ARE OUTLAYERS IN THE PRICE VARIABLE ARE THOSE ONE'S THAT LIVE IN A CITY WHERE THE HOUSES ARE MORE EXPENSIVE THAN THE OTHERS ONE. 

ALSO WE CAN OBSERVE THAT THE THE ONE'S THAT ARE OUTLAYERS ARE THOSE ONE'S THAT THE PRICE IS HIGHER THAN THE OVERALL MEAN (10 TIMES MORE), AND OBVIUSLY, HIS LIVING AREA IS MORE EXPENSIVE, INCLUDING MORE BATHROOMS AND BEEDROMS, THAT MAKE SENSE, THE ONE'S THAT ARE MORE EXPENSIVE ARE THOSE ONE'S THAT ARE IN MORE EXPENSIVE AREAS AND ALSO ARE MORE BIGGER THAN THE NORMAL ONE'S.

#variable 11 price_per_square_foot

This variable is interesting because, in future analyses, it could be very useful to know how much the price per square foot varies depending on the area or country. In this sense, it might be relevant to keep this variable for potential later studies.

However, from another point of view, if we already have the target variable price and also square_foot, the variable price_per_square_foot is simply a linear combination of these two, since it can easily be obtained by dividing the price by the number of square feet. Therefore, in this specific context, it does not provide new information or add significant value. Nevertheless, it could still be of interest for other types of analyses.

```{r}
sum_sq <- summary(df$pricePerSquareFoot)
boxplot(df$pricePerSquareFoot)

# LOOKING AT THIS, WEE HAVE A OUTLAYER, IN THAT CASE LET'S SEE WHICH IS THIS OUTLAYER. 
iqr <- sum_sq[5] - sum_sq[2]
llmild <- which(df$price < sum_sq[2] - 1.5*iqr | df$pricePerSquareFoot > sum_sq[5] + 1.5*iqr) 
llsev <- which(df$price < sum_sq[2] - 3*iqr | df$pricePerSquareFoot > sum_sq[5] + 3*iqr) 
length(llmild); length(llsev)

# SO IN THAT CASE WE HAVE TWO OUTLAYERS, LET'S SEE THAT OUTLAYERS

boxplot(df$pricePerSquareFoot)
abline(h = sum_sq[2] - 1.5*iqr, col='green')
abline(h = sum_sq[5] + 1.5*iqr, col='green')
abline(h=sum_sq[2] - 3*iqr, col='red')
abline(h=sum_sq[5] - 3*iqr, col='red')

sort(df$pricePerSquareFoot[llsev])

which(df$pricePerSquareFoot == 205000) # THE OBSERVATION 1273
df$price[1273]
df$livingArea[1273] # 1 m^2 for living....not logic.

# WE HAVE THIS TWO OUTLAYERS, SO LET'S SEE WHAT WE CAN DO WITH THESE OUTLAYERS.
# THOSE OUTLAYERS CORRESPON TO THAT ONE'S THAT ARE EXPENSIVE.

"Considering what we have mentioned before, we can remove this variable. Since we already have price and living area (which represents the square footage of the house), this value can be easily computed from those two variables."
df <- subset(df, select = -pricePerSquareFoot)
```

#variable 12 CITY.

We believe that City is an important variable because it allows for deeper analysis based on location. By considering the city, we can identify and compare which areas or regions tend to have higher or lower property prices. This information can provide valuable insights into geographic price patterns and local market dynamics. Therefore, it is important to keep this variable for future analysis.

```{r}
table(df$city)
prop.table(table(df$city))
sort(table(df$city))
sum(is.na(df$city)) # we don't have any missing values.

str(df$city)
df$city <- as.factor(df$city)
df$city
barplot(table(df$city))
```

#variable 13 state

In this case, all the properties in the dataset belong to the same state. Therefore, this variable does not provide any additional or differentiating information. Since it has no variability and is redundant, it can be safely removed from the dataset.

```{r}
which(df$state != "GA")
df <- subset(df, select = -state)
```


# variable 14 year_built

FOR THAT VARIABLE WE THINK THAT IT'S ALSO IMPORTANT TO KNOW FOR THE FURTHER ANALYSIS.
MAYBE, IN THE FUTURE, THE YEAR OF THE HOUSE IT'S SO IMPORTANT, SO THAT WILL MAKE A DIFFERENCE IN THE FINAL PRCIE, MAYBE NEW HOMES ARE MORE EXPENSIVE THAN THE OLDER ONE'S OR THE CONTRARY. 

```{r}
summary(df$yearBuilt)
boxplot(df$yearBuilt)

# we have also outlayers, so let's check carefully to see why these outlayers.

# DETECT THE OUTLAYERS

var_year <- summary(df$yearBuilt); var_year
iqr <- var_year[5] - var_year[2]; iqr

llmild <- which(df$yearBuilt < var_year[2] - 1.5*iqr | df$yearBuilt > var_year[5] + 1.5*iqr); llmild 
llsev <- which(df$yearBuilt < var_year[2] - 3*iqr | df$yearBuilt > var_year[5] + 3*iqr); llsev

boxplot(df$yearBuilt)
abline(v = var_year[2] - 1.5*iqr , col = 'green')
abline(v = var_year[5] + 1.5*iqr, col = 'green')
abline(v = var_year[2] - 3*iqr, col = 'red')
abline(v = var_year[5] + 3*iqr, col = 'red')

sort(df$yearBuilt[llsev])
# WE CAN SEE THAT THE ONE'S THAT ARE SEVERAL OUTLAYERS ARE THOSE ONE'S THAT ARE FROM 1800 - 1840, SO OLD HOUSES, AND ALSO A HOUSE THAT IS FOR THE YEAR 9999. MAYBE WE CAN REMOVE THAT VALUE BECAUSE MAKE NO SENSE HAVING A HOUSE OF THE YEAR 9999. MAYBE IS A WRONG VALUE. 

df$price[which(df$yearBuilt == 9999)] # THE PRICE FOR THIS HOME IS 285.750 €. Let's remain this value, and then in the further analysisi maybe we need to delete one.

idxx <- which(df$yearBuilt > 2021)
df$yearBuilt[idxx] <- NA

# NOW WE ARE GOING TO IMPUTE THOSE VALUES FOR THE YEAR BUILT. 
library(mice)
res.im <- mice(df[,c(7,9,10,11,12,13,14,15)])
df_i <- complete(res.im)

summary(df$yearBuilt); summary(df_i$yearBuilt)

df$yearBuilt <- df_i$yearBuilt
summary(df$yearBuilt)
```


# variable 15 STREET ADRESS. 

In this case, we consider the street address variable to be unimportant for our analysis. The specific street is not a determining factor in property prices; what truly matters are broader location variables such as the city and the zone.

For example, in Pedralbes, we know that all houses and apartments are generally very expensive, but the particular street within this area does not significantly influence the price. What really drives the price is the zone itself, in this case, Pedralbes. Therefore, we decided to remove this variable from the dataset, as it does not provide meaningful information for price prediction.

```{r}
df <- subset(df, select = -streetAddress)
```

# variable 16 / 17 / 18.

In this case, the reasoning is similar. We do not need the zip code, latitude, or longitude variables to predict or understand property prices. These variables do not provide additional or meaningful information for our analysis.

The zip code already indicates the city or area where the property is located, and that is sufficient for understanding geographic patterns in future analyses. As for latitude and longitude, they are not necessary for our purpose, since we already know the city and zone of each property. The exact geographic coordinates do not add relevant insight into price prediction, so we decided to remove these variables from the dataset.

```{r}
df <- subset(df, select= -c(zipcode, latitude, longitude))
```

# variable 19. hasbadGeocdoe

In this case, we found that this column contains the same value (0) for all observations. Since there is no variability in the data, this variable does not provide any meaningful or distinguishing information for our analysis.

A variable where all entries are identical cannot help in understanding differences between observations or in predicting the target variable. Therefore, we decided to remove this column from the dataset, as it is redundant and has no analytical value.

```{r}
which(df$hasBadGeocode != 0)
df <- subset(df, select = -hasBadGeocode)
```

# variable 20. Currency

In this case, we found that this column contains the same value (USD) for all observations. Since there is no variability in the data, this variable does not provide any meaningful or distinguishing information for our analysis.

A variable where all entries are identical cannot help in understanding differences between observations or in predicting the target variable. Therefore, we decided to remove this column from the dataset, as it is redundant and has no analytical value.


```{r}
table(df$currency)
prop.table(table(df$currency))
df <- subset(df, select = -currency)
```

# variable 21 Description

The description variable is not particularly relevant for our predictive analysis. Ultimately, we are interested in the objective features of the property, not its textual description. Therefore, we decided to remove this variable, as it does not provide any meaningful or useful information for our analysis.

```{r}
df<- subset(df, select =-description)
```


#variable 22 / 23 / 26 LIVING AREA & LIVING AREA VALUE & building area.

IN THAT CASE WE HAVE THREE VARIABLES THAT REPRESENTS THE SAME, SO HAVING BOTH OF THEM IS REDUNDANT SO IN THAT CASE WE WILL CHOOSE THE SECOND ONE, LIVING AREA VALUE.

```{r}
val <- c(0, 0)

for (i in 1:nrow(df)) {
  row <- df[i, c("livingArea", "livingAreaValue", "buildingArea")]
  if (length(unique(row)) == 1) {
    val[0] <- val[0] + 1  # número de filas con valores iguales
  } else {
    val[1] <- val[1] + 1  # número de filas con valores diferentes
  }
}
val;

# HERE WE CAN SEE THAT ALL THE VALUES ARE THE SAME IN EACH CATEGORY, SO ALL THE OBSERVATION IN THOSE 3 VARIABLES HAVE THE SAME VALUE, WE WILL STAY ONLY WITH ONE, livingArea. 
df <- subset(df, select = -livingAreaValue)
df <- subset(df, select = -buildingArea)

# ANALYZE THE LIVING AREA.
summary(df$livingArea)
boxplot(df$livingArea)
# OUTLAYERS.

var_liv <- summary(df$livingArea); var_liv
iqr <- var_liv[5] - var_liv[2]; iqr

llmild <- which(df$livingArea < var_liv[2] - 1.5*iqr | df$livingArea > var_liv[5] + 1.5*iqr); llmild
llsev <- which(df$livingArea < var_liv[2] - 3*iqr | df$livingArea > var_liv[5] + 3*iqr); llsev

boxplot(df$livingArea)
abline(h = var_liv[2] - 1.5*iqr,col = 'green')
abline(h = var_liv[5] + 1.5*iqr,col = 'green')
abline(h = var_liv[2] - 3*iqr, col = 'red')
abline(h = var_liv[5] + 3*iqr, col = 'red')

df[llsev,]

# ALSO LET'S SEE THE DISTRIBUTION OF THAT NUMERIC VARIABLE. 

hist(df$livingArea, freq = FALSE)
curve(dnorm(x,mean=mean(df$livingArea), sd= sd(df$livingArea)), lwd = 2, col = 'red', add = T)

# Applying logarithm

hist(log(df$livingArea), freq = FALSE)
curve(dnorm(x,mean=mean(log(df$livingArea)), sd= sd(log(df$livingArea))), lwd = 2, col = 'red', add = T)

```


# variable 24 BATHROOMS


```{r}
var_out <- summary(df$bathrooms)
boxplot(df$bathrooms)

iqr <- var_out[5] - var_out[2]

llm <- which(df$bathrooms < var_out[2] - 1.5*iqr | df$bathrooms > var_out[5] + 1.5*iqr); length(llm)
lls <- which(df$bathrooms < var_out[2] - 3*iqr | df$bathrooms > var_out[5] + 3*iqr); length(lls)

boxplot(df$bathrooms)
abline(h= var_out[2] - 1.5*iqr, col= 'green')
abline(h= var_out[5] + 1.5*iqr, col= 'green')
abline(h= var_out[2] - 3*iqr, col= 'red')
abline(h= var_out[5] + 3*iqr, col='red')

# tenim un total de 
length(llm) + length(lls) # -> 554 outlaiers. 520 mild i 34 de extreme. 

# Let's see the distribution of that variable

hist(df$bathrooms, freq = FALSE)
curve(dnorm(x, mean=mean(df$bathrooms), sd=sd(df$bathrooms)), lwd = 2, col = 'red', add = T)

# FOR THOSE WHO HAS 0 BATHROOMS, LET'S PUT THE NA VALUE AND THEN LET'S IMPUTE THEM. THERE ARE SO FEW WHO HAS NOT BATHROOM.
indx <- which(df$bathrooms == 0); indx
df$bathrooms[indx] <- NA
library(mice)
res.im <- mice(df[,c(7,9,10,11,12,13,14,15)])
df_im <- complete(res.im)

summary(df$bathrooms);summary(df_im$bathrooms)

# WE CAN SEE THAT THE DISTRIBUTION REMAIN EQUALLY, SO THAT IMPUTATION IS CORRECT. 

df$bathrooms <- df_im$bathrooms
summary(df$bathrooms)
```


# variable 25 BEEDROMS

```{r}
var_out <- summary(df$bedrooms)
boxplot(df$bedrooms)

iqr <- var_out[5] - var_out[2]

llm <- which(df$bedrooms < var_out[2] - 1.5*iqr | df$bedrooms > var_out[5] + 1.5*iqr); length(llm)
lls <- which(df$bedrooms < var_out[2] - 3*iqr | df$bedrooms > var_out[5] + 3*iqr); length(lls)

boxplot(df$bedrooms, horizontal = TRUE)
abline(v= var_out[2] - 1.5*iqr, col= 'green')
abline(v= var_out[5] + 1.5*iqr, col= 'green')
abline(v= var_out[2] - 3*iqr, col= 'red')
abline(v= var_out[5] + 3*iqr, col='red')


# tenim un total de 
length(llm) + length(lls) # $413 outlayers. 

# let's see the one's that are outlayers, why they are

df[lls,]
table(df$homeType[lls]) # MOst of them are from single family.

# NOW LET'S SEE THE DISTRIBUTION:

hist(df$bedrooms, freq = FALSE)
curve(dnorm(x,mean=mean(mean(df$bedrooms)), sd= sd(df$bedrooms)), lwd = 2, col = 'red', add = T)

# SEEMS TO BE SO NORMALLY DISTRIBUITED.
```



# variable 27 / 28 / 29  PARKING, HAS GARAGE , GARAGESPACES

HERE WE HAVE 3 VARIABLES THAT TALKS ABOUT THE PARKING, IN THAT CASE, WHAT IT'S REALLY IMPORTANT IS TO KNOW HOW MANY PLACES OF PARKING YOU HAVE, BECAUSE EACH PLACE COSTS MONEY, AND HAVING ONE OR TWO PLACES FOR PARKING, CHANGES THE PRICE, OBVIUSLY. ANDD HAVING NO ONE ALSO CHANGE THE PRICE, BUT IN THAT CASE WE HAVE THE 0 VALUE THAT REPRESENTS NO PARKING PLACE, AND THEN THE NUMBEROF ARKINGS THAT EACH HOUSE HAVE IT'S REALLY IMPORTANT AS WE MENTIONED ABOVE. 

IN ALL THIS 3 VARIABLES WE HAVE INCONSITENCY, SO WHICH ONE WE'LL CHOOSE ?? 

**REVISAR** QUÉ FEM PER SOLUCIONAR AQUESTES INCOSTIÈNCIES.

```{r}

for(i in 1:nrow(df)){
  parking <- df$parking[i]
  has_garage <- df$hasGarage[i]
  garagespaces <- df$garageSpaces[i]
  
  if(parking == 0 & has_garage == 0 & garagespaces >0){
    df$garageSpaces[i] <- 0
  } else if (parking == 0 & has_garage > 0){
    if(garagespaces > 0) df$parking[i] <- 1
    else df$hasGarage[i] <- 0
  } else if (parking > 0 & has_garage == 0){
    if(garagespaces > 0) df$hasGarage[i] <- 1
    else df$parking[i] <- 0
  } else if(parking > 0 & has_garage > 0 & garagespaces == 0){
    df$garageSpaces[i] <- NA
  }
}

summary(df$parking); summary(df$hasGarage); summary(df$garageSpaces)

# IN CASE WE HAVE NA VALUES DUE TO THE INCONSITECNY OF THAT 3 VARIABLES, LET'S IMPUTE THEM USING MICE.

library(mice)
res.imp <- mice(df[,c(7,9,10,11,12,13,14,15)])
df_imp <- complete(res.imp)

quantile(df$parking, na.rm = TRUE)
quantile(df_imp$parking)

quantile(df$hasGarage, na.rm = TRUE)
quantile(df_imp$hasGarage)

quantile(df$garageSpaces, na.rm = TRUE)
quantile(df_imp$g)

summary(df$parking); summary(df_imp$parking)
summary(df$hasGarage); summary(df_imp$hasGarage)
summary(df$garageSpaces); summary(df_imp$garageSpaces)

# WE LOOK HERE WITH THAT VALIDATION THAT THE DISTRIBTUION IT REMAINS ALMOST THE SAME

# LET'S SEE THE DISTRIBUTION IN THE GARAGESPACES.

df$garageSpaces <- df_imp$garageSpaces

summary(df$garageSpaces)

hist(df$garageSpaces, freq = FALSE)
curve(dnorm(x,mean=mean(df$garageSpaces),sd=sd(df$garageSpaces)), lwd = 2, col = 'red',add= T)

# WE LOOK THAT THIS IS NOT FOLLOWING A NORMAL DISTRIBTUION.

hist(log(df$garageSpaces), freq = FALSE)

# APPLYING THE LOGARITMIC TRANSFORMATION NEITHER.

hist(sqrt(df$garageSpaces), freq = FALSE)

# APPLYING THE SQRT TRANSOFMRATION NEITHER.

```


# variable 30 LEVELS

THAT VARIABLE IS IMPORTANT, BECAUSE CAN GIVE TO US INFORMATION ABOUT WHICH LEVEL HAS EACH HOME.

```{r}
table(df$levels)
prop.table(table(df$levels))
barplot(table(df$levels))
pie(table(df$levels))
sum(is.na(df$levels)) # WE DON'T HAVE ANY MISSING IN THAT CASE.
```

# VARIABLE 31 POOL

THAT IS SO IMPORTANT FOR THE PRICE, HAVING OR NO POOL IN THE HOUSE CAN CHANGE A LOT THE PRCIE OF THE HOME, SO IT'S AN IMPORTANT VARIABLE.

THE ONLY THINK THAT WE MUST TAKE INTO ACCOUNT IS THAT IS A NUMERIC (INT) VARIABLE, AND WE SHOULD CONVERT THAT TYPE OF DATA TO A FACTOR -> YES OR NO HAVE POOL, IT WILL BE EASIER TO WORK WITH THAT FACTOR VARIABLE. 

```{r}
str(df$pool)
which(df$pool < 0 | df$pool > 1) # we don't have any strange value
df$pool <- factor(df$pool, labels = c("NO", "YES"))

table(df$pool)
prop.table(table(df$pool))

barplot(table(df$pool)); pie(table(df$pool))
```


# VARIABLE 32 SPA

IN THAT VARIABLE HAPPENS THE SAME AS THE VARIABLE ABOVE, BECAUSE WHAT WE HAVE IS THE BINARY VALUE OF 0 OR 1, INDICATING IF THAT HOUSE HAS OR NOT SPA. ALSO IT'S AN IMPORTANT VARIABLE, BECAUSE HAS INFLUENCE IN THE FINAL PRICE.

```{r}
str(df$spa)
which(df$spa < 0 & df$spa > 1)
df$spa <- factor(df$spa, labels = c("NO", "YES"))

table(df$spa)
barplot(table(df$spa)); pie(table(df$spa))

```


# VARIABLE 33 IS NEW CONSTRUCTION

THE SAME AS VARIABLES ABOVE. THE IMPORTANCE OF THAT VARIABLE MAY COME TO UNDERSTAND IF NEW HOUSES ARE MORE EXPENSIVE THAN THE OLDER ONE'S OR THE CONTRARY, SO WE THINK IT'S AN IMPORTANT VARIABLE.

```{r}
str(df$isNewConstruction)
which(df$isNewConstruction < 0 | df$isNewConstruction > 1)
df$isNewConstruction <- factor(df$isNewConstruction, labels = c("NO", "YES"))

table(df$isNewConstruction)
prop.table(table(df$isNewConstruction))

barplot(table(df$isNewConstruction)); pie(table(df$isNewConstruction))

```

# VARIABLE 34 has pets allowed
The same applies to this variable, just like in the previous case. In this situation, it’s unclear to what extent it could be considered relevant, perhaps it refers to whether pets are allowed or not. However, in the end, when you buy a house, you can do whatever you want with it, right? Or maybe the variable indicates whether the property is adapted or suitable for pets. To be reviewed.

```{r}
str(df$hasPetsAllowed)
which(df$hasPetsAllowed < 0 | df$hasPetsAllowed > 1)
which(df$hasPetsAllowed != 0)
df$hasPetsAllowed <- factor(df$hasPetsAllowed, labels = c("NO", "YES"))

table(df$hasPetsAllowed)
prop.table(table(df$hasPetsAllowed))

```

# VARIABLE 35 HOME_TYPE

```{r}
str(df$homeType)
sum(is.na(df$homeType)) # no missing values.

"WE MUST ALSO CONVERT THAT VARIABLE TO A FACTOR VARIABLE"
df$homeType <- as.factor(as.character(df$homeType))

table(df$homeType)
prop.table(table(df$homeType))
"WE have here that the most houses are from the single family --> 88% aprox SO WE CAN GROUP THE OTHER ONE'S IN A GROUP THAT CAN BE KNOWN AS OTHER"
df$homeType <- as.character(df$homeType)
df$homeType[which(df$homeType != "SINGLE_FAMILY")] <- "Other"
df$homeType <- as.factor(as.character(df$homeType))

table(df$homeType)
prop.table(table(df$homeType))

barplot(table(df$homeType)); pie(table(df$homeType))

```
# VARIABLE 36 COUNTY

We decided to keep this variable because it provides more meaningful information than county_id. Therefore, since we chose not to use county_id and to keep county instead, we will proceed to analyze this variable.

```{r}
str(df$county)
sum(is.na(df$county)) # We don't have any missing value. 

df$county <- as.factor(as.character(df$county))

table(df$county)
prop.table(table(df$county))
sort(table(df$county))
```


#   FIRST DATA ANALYSIS

One of the most important steps is to analyze whether the response variable —that is, the target variable, in this case price, follows a normal distribution or not.

From this point, we will perform both a graphical and a statistical analysis to assess the normality of the target variable price.
  
```{r}

hist(df$price, freq = FALSE)
curve(dnorm(x,mean=mean(df$price), sd= sd(df$price)), lwd = 2, col= 'red', add = T)
qqnorm(df$price)
qqline(df$price)
# IN THAT CASE WE CAN'T SAY THAT IS NORMALLY DISTRIBUITED.

"LET'S CHECK APPLAYING THE LOGARITMIC TRANSFORMATION"
hist(log(df$price), freq = FALSE)
curve(dnorm(x,mean=mean(df$price), sd= sd(df$price)), lwd = 2, col= 'red', add = T)
qqnorm(log(df$price))
qqline(log(df$price))

# WE CAN SEE THAT THE POINTS ARE MORE ADJUSTED THAN BEFORE, BUT NEITHER IS FOLLOWING A NORMALLY DISTRIBUTION
# LET'S SEE WITH THE SHAPIRO TEST USING A RANDOM SAMPLE. 

# SAMPLE OF 500 OBSERVATIONS
sample_df <- df[sample(nrow(df), 500), ]


hist(sample$price, freq = FALSE, breaks = 50)
curve(dnorm(x,mean=mean(sample$price), sd= sd(sample$price)), lwd = 2, col = 'red', add = T)

# NOW LET'S APPLY A TRANSFORMATION, LOGARITMIC TRANSFORMATION
hist(log(sample$price), freq = FALSE, breaks = 25)
curve(dnorm(x,mean=mean(log(sample$price)), sd= sd(log(sample$price))), lwd = 2, col = 'red', add = T)

shapiro.test(log(sample$price))
shapiro.test(sample$price)

# WE CAN CONSIDER THAT THE PRICE VARIABLE IS NOT FOLLOWING A NORMAL DISTRIBUTION, BUT LOOKING AT THE GRAPHICS WE CAN OBSERVE A LITTLE BIT OF NORMALIZATION.

```

# Detect multivariate outlayers. 

One of the most important tasks we must carry out is the detection of multivariate outliers, that is, unusual cases that fall outside the normal pattern of the data. These exceptional values should be carefully analyzed to understand what is happening and to determine whether they correspond to errors, atypical observations, or meaningful information within our dataset.

  **MULTIVARIATE OUTLAYERS**
  
```{r}
library(chemometrics)
res.mout <- Moutlier(df[,c(7,9,10,11,12,14)], quantile = 0.99)
plot(res.mout$md, res.mout$rd)
abline(v = res.mout$cutoff, col= 'red')
abline(h = res.mout$cutoff, col= 'red')
text(res.mout$md, res.mout$rd,
     labels = rownames(df),   # usa el ID de cada fila (o el vector que quieras)
     pos = 4,                 # posición del texto (4 = a la derecha del punto)
     cex = 0.7,               # tamaño del texto
     col = "darkgray")

"THERE ARE SOME MULTIVARIATE OUTLAYERS, BUT THE FACT IS THAT MOST OF THEM SEEMS TO BE VERY AGRUPATED, BUT IN FACT, THERE IS ONE   WHO ARE FAR AWAY FROM THE REST, THAT ONE MAYBE CAN CAUSE PROBLEMS IN THE FUTURE"
```

TAKING INTO ACCOUNT THE GRAPHIC, WE HAVE A HUGE, VERY HUGE MULTIVARIANT OUTLAYER. LET'S SEE WHICH ONE IS THAT ONE THAT IS THE MULTIVARIATE OUTLAYER.

```{r}
library(FactoMineR)
idxmout <- which(res.mout$md > res.mout$cutoff & res.mout$rd > res.mout$cutoff); idxmout; length(idxmout)
df$mvout <- 0
df$mvout[idxmout] <- 1
df$mvout <- factor(df$mvout, labels = c('MVOUT-no','MVOUT-yes'))

res.cat <- catdes(df[,c(3:23)], num.var = 21)
res.cat$quanti.var # THAT VARIABLE OF BEING MULTIVARIATE OUTLYER OR NOT IS ASSOCIATED WITH PRICE AND LIVING AREA MOSTLY, THE RESGT ARE PRETTY CLOSE TO 0
res.cat$category
res.cat$quanti

# WE CAN SEE THAT THE ONE'S THAT ARE IN THE MULTIVARIATE OUTLAIER ARE THOSE ONE'S WHO HAVE A HIGHER PRICE, IN THE GROUP OF MVOUT-yes WE CAN SEE THAT THE MEAN GROUP IS SO HIGH IN COMPARIISON OF THE OVERALL, AND ALSO THE LIVING AREA, IS HIGHER.

"ALSO FOR THE REST OF VARIABLES WE CAN SEE THAT THE VALUES ARE ALSO HIGHER IN COMPARISION WITH THE OVERALL MEAN"

"SO YES, THE ONE'S THAT ARE MULTIVARIATE OUTLAYERS ARE THOSE ONE'S WHO HAS THE PRICE, LIVING AREA, BATHROOMS, BEDROOMS HIGHER THAN THE OVERALL MEAN"

# LET'S SEE WHICH IS THE VARIABLE THAT IS FAR AWAY AND MAY BE PROBLEMATIC. -> 41
df$price[41]
df$livingArea[41]
df$bathrooms[41]
df$bedrooms[41]
```

# Errors and missing values

One of the most important steps in the data analysis process is correcting errors and handling missing values. It is essential to ensure at all times that the data we work with is of high quality and free from missing values.

For this reason, it is crucial to properly detect and address these issues. In this case, we will use a very important function that we have already worked with several times, from the FactoMineR package, called mice. This function allows us to perform missing value imputation, ensuring that we work with high-quality data that does not contain missing information.

```{r}
sum(is.na(df)) # WE OBSERVE HERE THAT WE DO NOT HAVE MISSING VALUES. 
summary(df)
```

# TEST FOR DISCARD SERIAL CORRELATION. 

```{r}
corr_mat <- cor(df[,c(9,10,11,12,14)])
corr_mat
plot(df[,c(9,10,11,12,14)])
```



# SECCIO DE DUBTES PER RESOLDRE

**REVISAR** AQUEST FRAGMENT DE CODI, PER A QUÈ ÉS??
```{r}
str(df)

df$datePostedString <- as.Date(df$datePostedString, format = "%Y-%m-%d")
df$datePostedString <- as.numeric(format(df$datePostedString, "%Y"))
length(df[which(df$datePostedString == 2021),]$datePostedString)
df$stateId <- as.factor(df$stateId)
which(df$stateId == '16')

# Donat que algunes de les columnes tenen els mateixos valors, les eliminarem perquè no ens proporcionene en informació rellevant.
df <- df[-c('hasBadGeocode', '')]
df 
View(df)
which(df$currency != 'USD')

```

**REVISAR** ESTE FRAGMENTO DE CODIGO ESTABA EN LA VARIABLE IS_BANKEDOWNED

**REVISAR**: Borramos la row entera, o solo imputamos su valor? debemos ver si el individuo solo presenta ese error, o tambien tiene otras variables distintas.

